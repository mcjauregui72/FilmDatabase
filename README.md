# Film Recommendation Engine Project Demonstrating Extract, Transform, and Load

>SQL Python Connector is used to allow users to query from the SQL database in Python.
>
>Python libraries not already present can be loaded via GitBash. Load required libraries and modules by running ‘pip install -r requirements.txt’
>
>Required Python Libraries and Modules:  
> - pyodbc  
> - pandas  
> - json  
> - itertools  
> - emoji  
>
> Pyodbc also requires installing a psqlodbc_16_00_0000-x64 driver which can be found in this repository, under the Resources folder. Alternatively, psqlodbc_16_00_0000-x64.zip can be downloaded directly from [PostgreSQL ODBC Driver Download Page](https://www.postgresql.org/ftp/odbc/versions.old/msi/). After installing the driver, double click on the installer (the .msi or .exe file) and follow the prompts. Verify installation by reopening ODBC Data Sources (64-bit) via the Start menu. Go to the Drivers tab to confirm that PostgreSQL Unicode(x64) is now visible. 


## Project Goal - Film Recommendation Engine
Create a film recommendation engine to allow users to find films they might enjoy based on their preferences for the following:
- Director Name
- Actor Name
- Keyword
- Genre


The engine returns the top 10 films, in descending order of vote count, that contains the user input (i.e., director, actor, genre). 

This project focuses on the Extract, Transform, and Load processes rather than data visualization. As such, the user interface for the recommendation engine is a basic Python notebook that connects to PostgreSQL. 

The workflow diagram below gives a sequential overview of the steps taken to load and transform the data that would eventually become the .csv files (in red) used to create PostgreSQL tables.   


![Extract_transform_workflow_diagram](graphics/Extract_transform_workflow_diagram.png)


## Files

### Jupyter Notebooks
- extract_transform.ipynb: Jupyter notebook used to extract data from CSV files within the "Resources" folder
- odbc_query.ipynb: Jupyter notebook used to run a series of preset SQL queries using pyodbc
- odbc_user_input.ipynb: Jupyter notebook used to collect user inputs to use as parameters for SQL queries using pyodbc

### Folders
- csv_outputs: contains CSV files generated by the ETL process for importing into the SQL database
- graphics: contains other graphics used in this readme
- query outputs: saved queries used to test the functionality of the SQL database
- quick dbd: contains other files related to the quick dbd tool
- resources: contains source CSV files and the required ODBC driver for postgreSQL

## Extract Phase Goals  

### Some Ethical Considerations When Finding Data
Originally, the intent was to connect to a movie database API in to collect real-time data. However, the OMDB film API's free API key only allows users to perform 1,000 queries per day. Getting to a sizable data set would require running thousands of queries, because each film search is a single call. Options included running the extraction code over several days or creating multiple accounts to collect a sufficient amount of data. Both options were impractical and possibly an abuse of a free open API key.   
  
Instead, a Kaggle dataset of 5,000 films was identified and retrieved from a TMDB movie API, already loaded into a CSV file format. Notably, the creator of the Kaggle data source originally used IMDB's API but had switched to TMDB's API instead as publishing the dataset using IMDB had been found to violate IMDB's usage policy. These issues highlight the responsibility of a data engineer to follow policies of an API's publisher.


### Extraction Steps
The goal of the extract phase was to get familiar with the content and structure of the data, and identify which fields would need to be create to produce database tables needed for the recommendation engine.  
From the files involved, it is clear that the data contains lists of dictionaries.

Single records from the tmdb_5000_movies.csv file contain information on each film:  
>budget,genres,homepage,id,keywords,original_language,original_title,overview,popularity,production_companies,production_countries,release_date,revenue,runtime,spoken_languages,status,tagline,title,vote_average,vote_count
>237000000,"[{""id"": 28, ""name"": ""Action""}, {""id"": 12, ""name"": ""Adventure""}, {""id"": 14, ""name"": ""Fantasy""}, {""id"": 878, ""name"": ""Science Fiction""}]",http://www.avatarmovie.com/,19995,"[{""id"": 1463, ""name"": ""culture clash""}, {""id"": 2964, >""name"": ""future""}, {""id"": 3386, ""name"": ""space war""}, {""id"": 3388, ""name"": ""space colony""}, {""id"": 3679, ""name"": ""society""}, {""id"": 3801, ""name"": ""space travel""}, {""id"": 9685, ""name"": ""futuristic""}, {""id"": 9840, ""name"": >""romance""}, {""id"": 9882, ""name"": ""space""}, {""id"": 9951, ""name"": ""alien""}, {""id"": 10148, ""name"": ""tribe""}, {""id"": 10158, ""name"": ""alien planet""}, {""id"": 10987, ""name"": ""cgi""}, {""id"": 11399, ""name"": ""marine""}, {""id"": 13065, >""name"": ""soldier""}, {""id"": 14643, ""name"": ""battle""}, {""id"": 14720, ""name"": ""love affair""}, {""id"": 165431, ""name"": ""anti war""}, {""id"": 193554, ""name"": ""power relations""}, {""id"": 206690, ""name"": ""mind and soul""}, {""id"": 209714, >""name"": ""3d""}]",en,Avatar,"In the 22nd century, a paraplegic Marine is dispatched to the moon Pandora on a unique mission, but becomes torn between following orders and protecting an alien civilization.",150.437577,"[{""name"": ""Ingenious Film Partners"", ""id"": >289}, {""name"": ""Twentieth Century Fox Film Corporation"", ""id"": 306}, {""name"": ""Dune Entertainment"", ""id"": 444}, {""name"": ""Lightstorm Entertainment"", ""id"": 574}]","[{""iso_3166_1"": ""US"", ""name"": ""United States of America""}, {""iso_3166_1"": >""GB"", ""name"": ""United Kingdom""}]",2009-12-10,2787965087,162,"[{""iso_639_1"": ""en"", ""name"": ""English""}, {""iso_639_1"": ""es"", ""name"": ""Espa\u00f1ol""}]",Released,Enter the World of Pandora.,Avatar,7.2,11800  

Single records from the tmdb_5000_credits.csv file related to each film's cast and crew:  
>movie_id,title,cast,crew,19995, Avatar,"[{""cast_id"": 242, ""character"": ""Jake Sully"", ""credit_id"": ""5602a8a7c3a3685532001c9a"", ""gender"": 2, ""id"": 65731, ""name"": ""Sam Worthington"", ""order"": 0}, {""cast_id"": 3, ""character"": ""Neytiri"", >""credit_id"": ""52fe48009251416c750ac9cb"", ""gender"": 1, ""id"": 8691, ""name"": ""Zoe Saldana"", ""order"": 1}, {""cast_id"": 25, ""character"": ""Dr. Grace Augustine"", ""credit_id"": ""52fe48009251416c750aca39"", ""gender"": 1, ""id"": 10205, ""name"": >""Sigourney Weaver"", ""order"": 2}, {""cast_id"": 4, ""character"": ""Col. Quaritch"", ""credit_id"": ""52fe48009251416c750ac9cf"", ""gender"": 2, ""id"": 32747, ""name"": ""Stephen Lang"", ""order"": 3}, {""cast_id"": 5, ""character"": ""Trudy Chacon"", >""credit_id"": ""52fe48009251416c750ac9d3"", ""gender"": 1, ""id"": 17647, ""name"": ""Michelle Rodriguez"", ""order"": 4}, {""cast_id"": 8, ""character"": ""Selfridge"", ""credit_id"": ""52fe48009251416c750ac9e1"", ""gender"": 2, ""id"": 1771, ""name"": ""Giovanni >Ribisi"", ""order"": 5}, {""cast_id"": 7, ""character"": ""Norm Spellman"", ""credit_id"": ""52fe48009251416c750ac9dd"", ""gender"": 2, ""id"": 59231, ""name"": ""Joel David Moore"", ""order"": 6}, {""cast_id"": 9, ""character"": ""Moat"", ""credit_id"": >""52fe48009251416c750ac9e5"", ""gender"": 1, ""id"": 30485, ""name"": ""CCH Pounder"", ""order"": 7}, {""cast_id"": 11, ""character"": ""Eytukan"", ""credit_id"": ""52fe48009251416c750ac9ed"", ""gender"": 2, ""id"": 15853, ""name"": ""Wes Studi"", ""order"": 8}, >""cast_id"": 10, ""character"": ""Tsu'Tey"", ""credit_id"": ""52fe48009251416c750ac9e9"", ""gender"": 2, ""id"": 10964, ""name"": ""Laz Alonso"", ""order"": 9}, {""cast_id"": 12, ""character"": ""Dr. Max Patel"", ""credit_id"": ""52fe48009251416c750ac9f1"", >""gender"": 2, ""id"": 95697, ""name"": ""Dileep Rao"", ""order"": 10}, {""cast_id"": 13, ""character"": ""Lyle Wainfleet"", ""credit_id"": ""52fe48009251416c750ac9f5"", ""gender"": 2, ""id"": 98215, ""name"": ""Matt Gerald"", ""order"": 11}, {""cast_id"": 32, >""character"": ""Private Fike"", ""credit_id"": ""52fe48009251416c750aca5b"", ""gender"": 2, ""id"": 154153, ""name"": ""Sean Anthony Moran"", ""order"": 12}, {""cast_id"": 33, ""character"": ""Cryo Vault Med Tech"", ""credit_id"": ""52fe48009251416c750aca5f"", >""gender"": 2, ""id"": 397312, ""name"": ""Jason Whyte"", ""order"": 13}, {""cast_id"": 34, ""character"": ""Venture Star Crew Chief"", ""credit_id"": ""52fe48009251416c750aca63"", ""gender"": 2, ""id"": 42317, ""name"": ""Scott Lawrence"", ""order"": 14}, >{""cast_id"": 35, ""character"": ""Lock Up Trooper"", ""credit_id"": ""52fe48009251416c750aca67"", ""gender"": 2, ""id"": 986734, ""name"": ""Kelly Kilgour"", ""order"": 15}, {""cast_id"": 36, ""character"": ""Shuttle Pilot"", ""credit_id"": ""52fe48009251416c750aca6b"", ""gender"": 0, ""id"": 1207227, ""name"": ""James Patrick Pitt"", ""order"": 16}, {""cast_id"": 37, ""character"": ""Shuttle Co-Pilot"", ""credit_id"": ""52fe48009251416c750aca6f"", ""gender"": 0, ""id"": 1180936, ""name"": ""Sean Patrick Murphy"", ""order"": 17}, {""cast_id"": 38, ""character"": ""Shuttle Crew Chief"", ""credit_id"": ""52fe48009251416c750aca73"", ""gender"": 2, ""id"": 1019578, ""name"": ""Peter Dillon"", ""order"": 18}, {""cast_id"": 39, ""character"": ""Tractor Operator / Troupe"", >""credit_id"": ""52fe48009251416c750aca77"", ""gender"": 0, ""id"": 91443, ""name"": ""Kevin Dorman"", ""order"": 19}, {""cast_id"": 40, ""character"": ""Dragon Gunship Pilot"", ""credit_id"": ""52fe48009251416c750aca7b"", ""gender"": 2, ""id"": 173391, ""name"": >""Kelson Henderson"", ""order"": 20}, {""cast_id"": 41, ""character"": ""Dragon Gunship Gunner"", ""credit_id"": ""52fe48009251416c750aca7f"", ""gender"": 0, ""id"": 1207236, ""name"": ""David Van Horn"", ""order"": 21}, {""cast_id"": 42, ""character"": ""Dragon >Gunship Navigator"", ""credit_id"": ""52fe48009251416c750aca83"", ""gender"": 0, ""id"": 215913, ""name"": ""Jacob Tomuri"", ""order"": 22}, {""cast_id"": 43, ""character"": ""Suit #1"", ""credit_id"": ""52fe48009251416c750aca87"", ""gender"": 0, ""id"": 143206, >""name"": ""Michael Blain-Rozgay"", ""order"": 23}, {""cast_id"": 44, ""character"": ""Suit #2"", ""credit_id"": ""52fe48009251416c750aca8b"", ""gender"": 2, ""id"": 169676, ""name"": ""Jon Curry"", ""order"": 24}, {""cast_id"": 46, ""character"": ""Ambient Room >Tech"", ""credit_id"": ""52fe48009251416c750aca8f"", ""gender"": 0, ""id"": 1048610, ""name"": ""Luke Hawker"", ""order"": 25}, {""cast_id"": 47,...   

The .csv files were read into VS Code as data frames ‘movies’ and ‘credits.’  

The tmdb_5000_movies.csv file was filtered for English-language movies only, as the characters in some foreign-language film titles were not translated.   

The tmdb_5000_credits.csv file was filtered for the cast and crew corresponding to only the English-language films retained from the first file.  

![readme01](graphics/readme01.png)

![readme02](graphics/readme02.png)

![readme03](graphics/readme03.png)

![readme04](graphics/readme04.png)

## Transform Phase Goals  
The goal of the transform phase was to parse the imported data in ways that would allow for creating a series of data frames. These data frames were later export as .csv files and imported into the tables of a SQL database. A SQL database was chosen because there are many items for each film that contain multiple entries such as genre and actor. The schema below indicates the relationships establish between nine database tables.   

![QuickDBD_Project3](graphics/QuickDBD_Project3.png)

The first data frame created was the ‘genres_df’ data frame. First, ‘ids_names’ were defined as an empty set for the genre ids and names found in The Movie Database. This set was filled by looping through the ‘genres’ column of the ‘en_movies’ data frame (English-language films), and isolating ‘id’ and ‘name’ from each row. The json.loads(row) method was used to parse the JSON string of ’id’ and ‘name’ data into a Python dictionary. This afforded access to the data as key-value pairs, with ‘id’ and ‘name’ containing the film genre id and film genre name. Next IDs were converted from strings to integers.

Adding in the emojis for each ‘genre’ value required defining a dictionary of emojis, ‘emoji_dict’.
From the items in ‘emoji_dict’, an ‘emoji_df’ dataframe was created to also included the genres’ ‘alias’. A left merge on ‘emoji_genres’ and ‘emoji_df’ on the column ‘genre allowed for populating the ‘emoji_genres’ dataframe. isLt comprehension was performed to apply the emoji.emojize() function to each emoji alias value and generate the emoji character from the alias language. Finally, the ‘emoji_genres’ dataframe was exported as a .csv file called 'emoji_genres.csv'

![readme05](graphics/readme05.png)

![readme06](graphics/readme06.png)

![readme07](graphics/readme07.png)

![readme08](graphics/readme08.png)

![readme09](graphics/readme09.png)

To create a data frame from the .csv file for the ‘actors’, consideration of characters (actors) were restricted to only the first five listed per film. The ‘credits’ data frame was filtered for the first five cast members present per movie within the ‘cast’ field.  Along with each uniquely-identified movie_id, each ‘cast’ dictionary (‘cast_id’ and ‘character’) embedded within the ‘cast’ field of the credits dataframe was saved. The result was a data frame containing five instances of the movie_id, one for each of the first five cast members credited in the film. 

The example below shows a test for the first five actor ids and names associated with the movie Avatar, the first film in the ‘credits’ dataframe. The subsequent screenshots illustrate how thi process was performed on the entire dataset.   

The itterows method in pandas iterated over the rows of the ‘cast’ data frame, to split out actor names from each movie title into a list. A similar process isolated the ‘movie_id’ values. An empty set and empty list were initialized to capture the unique actor ids (as any actor could appear multiple times in the data set for being in multiple movies).   

A for loop and and several if statements iterated through the cast data to get only one instance of each actor name and actor id appearing in the data set. The loop was defined to stop working once the number of unique actors per film reached 5.    

Separate lists were created for ‘actor_id’ and ‘actor_name.’ The ‘five_df’ data frame was built to hold ‘movie_id’, ‘actor_id’, and ‘actor_name.’ A new data frame called 'credits_actors_name' was created by appending the five_df dataframe to the credits_actor_name, which contains all actor-related data for all films in the datasets. 'actor_id' was transformed from string to integer. 


![readme10](graphics/readme10.png)

![readme11](graphics/readme11.png)

![readme12](graphics/readme12.png)


Creating the ‘credits_actor_df’ data frame by dropping actor ‘name’ from ‘credits_actor_name’ produced the data needed to create the future ‘credit_actor’ table needed to link ‘actor’ and ‘movies’ tables in the SQL database. The date in ‘credits_actor_df’ was exported as a .csv for later use.   


![readme13](graphics/readme13.png)

To create an ‘actor’ dataframe, .csv file, and database table of ‘actor_id’ and ‘actor_name’, ‘ids_cast’ was defined as a set to hold unique pairs of ‘actor_id' and actor ’name’. Iterating through the ‘credits_actor_name’ data frame using the iterrows() method enabled extraction of ‘actor_id’ and actor ‘name’ from the rows and placed them in the ‘ids_cast’ set. Using the itertools.islice() method on the ‘ids_cast’ set made it possible to print out the first 10 ‘actor_id’ and ‘actor_name’ pairs to the terminal.   


![readme14](graphics/readme14.png)


![readme15](graphics/readme15.png)


Identifing each film's director was a matter of first extracting the ‘crew’ and the ‘movie_id’ information from the credits data file and saving it in a data frame called ‘movie_crew’. Next, the .iterrows(); method iterated through ‘movie_crew’ and loaded the data found in the ‘crew’ row as a Python list to ‘data_list’. Similarly, .iterrows() isolated the ‘movie_id’ value from the ‘movie_id’ row.   

Within the ‘data_list’, it was necessary to identify crew_members with a ‘job’ value equal to “Director.” This was accomplished by using a for loop on the ‘data_list’. Next, .get() appended each Director’s ‘id’ to an empty list called ‘director_ids’.   

Creating a new dataframe called ‘movie_director’ allowed for associating a list of ‘movie_id’s with their related ‘director_id’s. Because multiple directors can be credited per movie, it became necessary to concatenated an empty dataframe ‘movieid_directorid_df with ‘movie_director’ and convert movie and director id fields to integers with .astype(). Finally, ‘movieid_directorid_df’ was exported to a .csv file for later use in the SQL database table creation.   


![readme16](graphics/readme16.png)


![readme17](graphics/readme17.png)


Linking the ‘directors’ table to the ‘movies’ table in the eventual SQL database required building a table containing ‘director_id’ and ‘movie_id’. With the credits datafile containing ‘movie_id’ and ‘crew’, the iterrows(): method isolated ‘crew’ and ‘movie_id.’ The .get() method identified instances where ‘job’ equals “Director,” making it possible to append all (director) ‘id’ values to an empty list called ‘director_ids’. This list was used to create the ‘movie_director’ data frame to pair ‘movie_id’s with associated ‘director_id’s.    

An initialized data frame ‘movieid_directorid_df’ was concatenated with the ‘movie_director’ data frame before converting ‘movie_id’s and ‘director_id’s to integers with astype(int). Next, ‘movieid_directorid_df’ was exported as a .csv file.   


![readme18](graphics/readme18.png)

To allow users to search for films based on keywords, a 'keywords' field from the 'en_movies' dataset (created from the original 'movies' and 'crew' data files) had to be included in the SQL database. 

The for loop-json.loads() process helped isolate (keyword) ‘id’ and (keyword) ‘name’ from the source data for the defined ‘ids_keyword’ set. Again, the resulting data frame, ‘keywords_df’ was exported as a .csv file for import into the SQL database. 


![readme19](graphics/readme19.png)


![readme20](graphics/readme20.png)


A query-by-keywords function for the film recommendation engine required a data frame, .csv file, and database table linking keywords with movie ids. This link was established with a similar process used to create the other linking data frames/.csv files/tables. The script is shown below.


![readme21](graphics/readme21.png)


![readme22](graphics/readme22.png)


The final dataframe-to .csv-to table needed was for ‘movies’. Selected columns from the ‘en_movies’ data set were assigned to a dictionary called ‘data’. A data frame was then made from this dictionary. Within that data frame, all ‘movie_id’ values were converted from string to integer type using .astype(int). ‘release_date’ values were also converted from string to datetime type with the pd.to_datetime method. Null values were assigned to the ‘tagline’ and ‘release_date’ columns that were missing data. Finally, ‘movies_df’ was exported to a .csv file for further use. 


![readme23](graphics/readme23.png)


![readme24](graphics/readme24.png)


## Load Phase Goals

The goal of the Load Phase was to successfully import all 9 .csv files, create the tables into which the .csv files would be imported, and create the table joins needed to demonstrate the movie recommendation engine. To create an interactive experience with the SQL database, a simple but easy-to-use user interface was created with SQL Python Connector to allows users to query from the SQL database in Python.

The following queries were used to create the tables designed for the SQL database. 

-- Exported from QuickDBD: https://www.quickdatabasediagrams.com/  
-- Link to schema: https://app.quickdatabasediagrams.com/#/d/Hftcv0  
-- NOTE! If you have used non-SQL datatypes in your design, you will have to change these here.  

-- Exported from QuickDBD: https://www.quickdatabasediagrams.com/  
-- Link to schema: https://app.quickdatabasediagrams.com/  
-- NOTE! If you have used non-SQL datatypes in your design, you will have to change these here.  

CREATE TABLE "movies" (
    "movie_id" int   NOT NULL,
    "title" varchar(100)   NOT NULL,
    "revenue" bigint   NOT NULL,
    "tagline" varchar(300)   NOT NULL,
    "average_vote" numeric(5,1)   NOT NULL,
    "popularity" decimal(9,6)   NOT NULL,
    "release_date" date   NOT NULL,
    CONSTRAINT "pk_movies" PRIMARY KEY (
        "movie_id"
     )
);

CREATE TABLE "actor" (
    "actor_id" int   NOT NULL,
    "actor" varchar(50)   NOT NULL,
    CONSTRAINT "pk_actor" PRIMARY KEY (
        "actor_id"
     )
);

CREATE TABLE "credits_actor" (
    "movie_id" int   NOT NULL,
    "actor_id" int   NOT NULL
);

CREATE TABLE "directors" (
    "director_id" int   NOT NULL,
    "director" varchar(100)   NOT NULL,
    CONSTRAINT "pk_directors" PRIMARY KEY (

   "director_id"
     )
);

CREATE TABLE "movieid_director_id" (
    "movie_id" int   NOT NULL,
    "director_id" int   NOT NULL
);

CREATE TABLE "emoji_genre" (
    "genre_id" int   NOT NULL,
    "genre" varchar(100)   NOT NULL,
    "alias" varchar(100)   NOT NULL,
    "genre_emoji" varchar(100)   NOT NULL,
    CONSTRAINT "pk_emoji_genre" PRIMARY KEY (
        "genre_id"
     )
);

CREATE TABLE "movieid_genre_ids" (
    "movie_id" int   NOT NULL,
    "genre_id" int   NOT NULL
);

CREATE TABLE "keywords" (
    "keyword_id" int   NOT NULL,
    "keyword" varchar(50)   NOT NULL,
    CONSTRAINT "pk_keywords" PRIMARY KEY (
        "keyword_id"
     )
);

CREATE TABLE "movieids_kw" (
    "movie_id" int   NOT NULL,
    "keyword_id" int   NOT NULL
);

ALTER TABLE "credits_actor" ADD CONSTRAINT "fk_credits_actor_movie_id" FOREIGN KEY("movie_id")
REFERENCES "movies" ("movie_id");

ALTER TABLE "credits_actor" ADD CONSTRAINT "fk_credits_actor_actor_id" FOREIGN KEY("actor_id")
REFERENCES "actor" ("actor_id");

ALTER TABLE "movieid_director_id" ADD CONSTRAINT "fk_movieid_director_id_movie_id" FOREIGN KEY("movie_id")
REFERENCES "movies" ("movie_id");

ALTER TABLE "movieid_director_id" ADD CONSTRAINT "fk_movieid_director_id_director_id" FOREIGN KEY("director_id")
REFERENCES "directors" ("director_id");

ALTER TABLE "movieid_genre_ids" ADD CONSTRAINT "fk_movieid_genre_ids_movie_id" FOREIGN KEY("movie_id")
REFERENCES "movies" ("movie_id");

ALTER TABLE "movieid_genre_ids" ADD CONSTRAINT "fk_movieid_genre_ids_genre_id" FOREIGN KEY("genre_id")
REFERENCES "emoji_genre" ("genre_id");

ALTER TABLE "movieids_kw" ADD CONSTRAINT "fk_movieids_kw_movie_id" FOREIGN KEY("movie_id")
REFERENCES "movies" ("movie_id");

ALTER TABLE "movieids_kw" ADD CONSTRAINT "fk_movieids_kw_keyword_id" FOREIGN KEY("keyword_id")
REFERENCES "keywords" ("keyword_id");

Files were imported into tables, in this order:
1. actor  
2. directors  
3. emoji_genre  
4. keywords  
5. movies  
6. credits_actor  
7. movieid_director_id  
8. movieid_genre_ids  
9. movieids_kw 


## Running the movie recommendation engine
Steps to interact with the movie recommendation engine:
1. (If necessary) Go to the Resources folder and locate the psqlodbc_16_00_0000-x64 folder. Within the folder is an installer file called psqlodbc_x64. Run the installer. 
2. Open up VS Code and the folder containing the obdc_user_input.ipynb files. 
3. Run the first two blocks of code to import pyodbc and pandas, and connect to the Postgres SQL database.
4. Run the Actor, Genre, Director, Keyword, and Keyword (LIKE) blocks one by one.
   

![readme26](graphics/readme26.png)

![readme27](graphics/readme27.png)


The short video shows the finished Movie Recommendation Engine:

https://github.com/mcjauregui72/FilmDatabase/User_Input_Demo.mp4


## Future Improvements

The first main improvement possible for this project would be to retrieve data from a live API instead of a Kaggle dataset. Using a paid API key makes it possible to build a more repeatable Python script to periodically query the API and update the SQL database to keep it up to date with new films.   
  
Another improvement could be a standalone application that leverages ODBC or another database data retrieval method to give a list of film results in the form of a CSV or visualization tool.   
  
Finally, the recommendation engine's user queries are fairly simple. They accept only a single entry for a single field (actor, director, genre, or keyword). There is more room for performing multiple queries and using other film information to make recommendations.

## Sources:  

Kaggle source  
https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata/data?select=tmdb_5000_movies.csv  

TMDB   
https://developer.themoviedb.org/docs/getting-started  

For json.loads info  
https://www.geeksforgeeks.org/json-loads-in-python/  

For info on sets  
https://realpython.com/python-sets/  

For emoji library  
https://emoji-python.readthedocs.io/en/stable/  

For itertools:  
https://docs.python.org/3/library/itertools.html  

For String Aggregate  
https://www.postgresqltutorial.com/postgresql-aggregate-functions/postgresql-string_agg-function/  

Pyodbc:  
https://pypi.org/project/pyodbc/

For User Inputs with pyodbc:  
https://stackoverflow.com/questions/9518148/pyodbc-how-to-perform-a-select-statement-using-a-variable-for-a-parameter





